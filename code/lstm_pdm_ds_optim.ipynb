{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27059244",
   "metadata": {},
   "source": [
    "\n",
    "背景\n",
    "\n",
    "NASA的预测卓越中心（Prognostic Center of Excellence）建立了一个数据集仓库，用于为预测与预测性维护相关的算法提供基准测试。\n",
    "\n",
    "其中一个数据集来自涡扇发动机的仿真模型 C-MAPSS（Commercial Modular Aero Propulsion System Simulation，商业模块化航空推进系统仿真）。C-MAPSS 是一个用于生成模拟涡扇发动机的健康状态、控制参数和引擎参数的工具。通过一个自定义代码封装器，在传感器时间序列数据中注入了合成故障和持续退化趋势。\n",
    "\n",
    "该数据集的一些特征如下：\n",
    "\n",
    "数据来自高保真度的涡扇发动机仿真，但能够很好地模拟实际发动机的传感器数值；\n",
    "数据中加入了合成噪声，以模拟真实世界中的情况；\n",
    "故障效应受操作工况的掩盖，这在大多数真实系统中也是常见现象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08c30f",
   "metadata": {},
   "source": [
    "定义数据集的列\n",
    "\n",
    "在原始数据集中，每一行数据中都包含空格分开的各个列。其含义如下：\n",
    "\n",
    "id：        发动机编号\n",
    "cycle：     发动机运行次数\n",
    "setting1：  参数设置1\n",
    "...\n",
    "s1:         传感器1的数据\n",
    "...\n",
    "\n",
    "每一行代表每一次运行的数据。id和cycle表示的是第几号发动机第几次的运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'CMAPSSData'\n",
    "columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3','s4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14','s15', 's16', 's17', 's18', 's19', 's20', 's21']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c67ef2",
   "metadata": {},
   "source": [
    "从数据集的目录下加载数据文件\n",
    "\n",
    "数据集目录下文件的特点：\n",
    "train_FD001~4 : 4个训练用数据文件。包含每个发动机运行到坏的所有次数的数据\n",
    "test_FD001~4 ： 4个测试用数据文件。包含每个发动机运行到当前次数的数据\n",
    "RUL_FD001~4 :   4个标签数据。对应每个test文件中的当前RUL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4a6c8",
   "metadata": {},
   "source": [
    "下面代码的数据处理说明：\n",
    "\n",
    "从文件中读取训练数据\n",
    "\n",
    "每个文件中读取的行列数据放入pandas数据集。给数据集加上列头，然后把每个setting数据和传感器数据进行归一化处理，\n",
    "也就是：归一化的列值 =（列值-列最小值）/（列最大值-列最小值）\n",
    "\n",
    "把数据按id列进行分组group，即按不同发动机分组。分组的cycle取最大值，也就是该发动机最大的运行次数。\n",
    "最大值-每行cycle值 = 每行RUL值。计算出RUL后就drop掉max值。\n",
    "这样训练数据每一行就加入了标签数据RUL列\n",
    "\n",
    "从4个文件中读取的数据集全部加到train_df的数据里\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "train_df_raw = []\n",
    "\n",
    "eps = 0.000001 # for floating point issues during normalization \n",
    "for i in range(1,5):\n",
    "    df = pd.read_csv('CMAPSSData/train_FD{:03d}.txt'.format(i), sep='\\s+', header=None)\n",
    "    #df = pd.read_csv('CMAPSSData/train_FD{:03d}.txt'.format(i), delim_whitespace=True, header=None)\n",
    "    #df.drop(df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    df.columns = columns\n",
    "    df_raw = df.copy()\n",
    "\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    train_df.append(df)\n",
    "    train_df_raw.append(df_raw)\n",
    "\n",
    "# compute RUL (remaining useful life)\n",
    "for i, df in enumerate(train_df):\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left')\n",
    "    df['RUL'] = df['max'] - df['cycle']\n",
    "    df.drop('max', axis=1, inplace=True)\n",
    "    train_df[i]=df\n",
    "\n",
    "o = train_df[0][columns[20:]+['RUL']][train_df[0]['id'] == 1].plot(subplots=True, sharex=True, figsize=(20,10), title=\"Train: 8 sensors of Engine 1 before failure\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208877ac",
   "metadata": {},
   "source": [
    "原始数据和归一化的数据的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE=2          #数据文件编号，从0到3\n",
    "COLUMN=12        #数据列号，从2到24\n",
    "ENGINE_ID=100   #发动机编号，从1到每个文件最大的id\n",
    "\n",
    "\n",
    "pd.concat(\n",
    "    [train_df_raw[FILE][columns[COLUMN]][train_df_raw[FILE]['id'] == ENGINE_ID], \n",
    "     train_df[FILE][columns[COLUMN]][train_df[FILE]['id'] == ENGINE_ID]], \n",
    "     axis=1).plot(\n",
    "         subplots=True, \n",
    "         sharex=True, \n",
    "         figsize=(20,10), \n",
    "         title=\"Raw data and normalized datasequence\")\n",
    "\n",
    "\n",
    "#train_df_raw[FILE][columns[COLUMN]][train_df_raw[0]['id'] == ENGINE_ID].plot(subplots=False, sharex=True, figsize=(20,10), title=\"Raw data sequence\")\n",
    "#train_df[FILE][columns[COLUMN]][train_df[0]['id'] == ENGINE_ID].plot(subplots=False, sharex=True, figsize=(20,10), title=\"Nomalized data sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "for i in range(1,5):\n",
    "    # Load time series\n",
    "    df = pd.read_csv('CMAPSSData/test_FD{:03d}.txt'.format(i), sep='\\s+', header=None)\n",
    "\n",
    "    # Load the RUL values\n",
    "    df_rul = pd.read_csv('CMAPSSData/RUL_FD{:03d}.txt'.format(i), sep='\\s+', header=None)    \n",
    "    df_rul.index += 1\n",
    "\n",
    "    # Merge RUL and timeseries and compute RUL per timestamp\n",
    "    df = df.merge(df_rul, left_on=df.columns[0], right_index=True, how='left')\n",
    "\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df.columns = columns + ['RUL_end']\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left') # We get the number of cycles per series\n",
    "    df['RUL'] = df['max'] + df['RUL_end'] - df['cycle'] # The RUL is the number of cycles per series + RUL - how many cycles have already ran\n",
    "    df.drop(['max','RUL_end'], axis=1, inplace=True)\n",
    "    \n",
    "    # Normalize\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    test_df.append(df)\n",
    "\n",
    "\n",
    "t = test_df[0][columns[20:]+['RUL']][test_df[0]['id'] == 1].plot(subplots=True, sharex=True, figsize=(20,10), title=\"Test: sensors and RUL before failure\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 假设你只取 sensor 和 setting 数据，不包括 'id', 'cycle', 'RUL'\n",
    "feature_cols = columns[2:]  # ['setting1', ..., 's21']\n",
    "target_col = 'RUL'\n",
    "\n",
    "def df_to_sequence_list(df, feature_cols, target_col=None):\n",
    "    \"\"\"\n",
    "    将 dataframe 转为 [batch, seq_len, feature] 形式的列表\n",
    "    返回两个 list：features, labels\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    for _, group in df.groupby('id'):\n",
    "        feature_tensor = torch.tensor(group[feature_cols].values, dtype=torch.float32)\n",
    "        sequences.append(feature_tensor)\n",
    "\n",
    "        if target_col is not None:\n",
    "            label_tensor = torch.tensor(group[target_col].values, dtype=torch.float32)\n",
    "            labels.append(label_tensor)\n",
    "\n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 train_df 处理\n",
    "train_sequences, train_labels = [], []\n",
    "for df in train_df:\n",
    "    seqs, lbls = df_to_sequence_list(df, feature_cols, target_col)\n",
    "    train_sequences.extend(seqs)\n",
    "    train_labels.extend(lbls)\n",
    "\n",
    "# 对 test_df 处理\n",
    "test_sequences, test_labels = [], []\n",
    "for df in test_df:\n",
    "    seqs, lbls = df_to_sequence_list(df, feature_cols, target_col)\n",
    "    test_sequences.extend(seqs)\n",
    "    test_labels.extend(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_sequences = pad_sequence(train_sequences, batch_first=True)  # [batch, max_seq_len, feature]\n",
    "padded_train_labels = pad_sequence(train_labels, batch_first=True)        # [batch, max_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sequences))\n",
    "print(len(train_labels))\n",
    "print(train_sequences[0].shape)\n",
    "print(train_labels[0].shape)\n",
    "\n",
    "batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66912ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_sequences[batch][:,20:].numpy()  # 形状 (192, 2)\n",
    "p = pd.DataFrame(a)\n",
    "p.plot( subplots=False, sharex=True, figsize=(20,10), title=\"Train: sensors before failure\")\n",
    "\n",
    "a = train_labels[batch][:].numpy()  # 形状 (192, 2)\n",
    "p = pd.DataFrame(a)\n",
    "p.plot( subplots=False, sharex=True, figsize=(20,10), title=\"Train: RUL before failure\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_train_sequences.shape)\n",
    "print(padded_train_labels.shape)\n",
    "\n",
    "a = padded_train_sequences[batch, :,20:].numpy()\n",
    "p = pd.DataFrame(a)\n",
    "p.plot( subplots=False, sharex=True, figsize=(20,10), title=\"Train: sensors before failure\")\n",
    "\n",
    "a = padded_train_labels[batch, :].numpy()\n",
    "p = pd.DataFrame(a)\n",
    "p.plot( subplots=False, sharex=True, figsize=(20,10), title=\"Train: RUL before failure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bea227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = torch.tensor([len(seq) for seq in train_sequences])  # [batch]\n",
    "test_lengths = torch.tensor([len(seq) for seq in test_sequences])    # [batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd65e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ConvLSTMRULPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, conv_channels=64, lstm_hidden_dim=128, num_layers=2, dropout=0.2):\n",
    "        super(ConvLSTMRULPredictor, self).__init__()\n",
    "        \n",
    "        # 第一层卷积\n",
    "        self.conv1 = nn.Conv1d(input_dim, conv_channels, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(conv_channels)\n",
    "        \n",
    "        # 第二层卷积\n",
    "        self.conv2 = nn.Conv1d(conv_channels, conv_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(conv_channels)\n",
    "        \n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(lstm_hidden_dim*2, 1)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # x shape: [batch, seq_len, features]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 转置卷积维度 [batch, features, seq_len]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # 第一层卷积 + ReLU + BatchNorm\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # 第二层卷积 + ReLU + BatchNorm\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # 转回LSTM需要的维度 [batch, seq_len, features]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # 打包序列以处理变长序列\n",
    "        packed_input = pack_padded_sequence(\n",
    "            x, \n",
    "            lengths.cpu(), \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # LSTM处理\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        \n",
    "        # 解包序列\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # 输出层\n",
    "        out = self.fc(output)\n",
    "        \n",
    "        return out.squeeze(-1)  # [batch, seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d35b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数\n",
    "input_dim = padded_train_sequences.shape[2]  # 特征维度\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ConvLSTMRULPredictor(\n",
    "    input_dim=input_dim,\n",
    "    conv_channels=64,\n",
    "    lstm_hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "#criterion = nn.MSELoss()  # 回归问题使用MSE损失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_result_array=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_model(model, train_sequences, train_labels, train_lengths, num_epochs=50, batch_size=32):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # 随机打乱数据\n",
    "        indices = torch.randperm(len(train_sequences))\n",
    "        \n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            \n",
    "            batch_sequences = train_sequences[batch_indices]\n",
    "            batch_labels = train_labels[batch_indices]\n",
    "            batch_lengths = train_lengths[batch_indices]\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(batch_sequences, batch_lengths)\n",
    "            \n",
    "            # 计算RMSE损失（逐样本计算后取平均）\n",
    "            batch_loss = 0\n",
    "            for j in range(len(batch_lengths)):\n",
    "                seq_len = batch_lengths[j]\n",
    "                # 1. 计算每个时间步的平方误差\n",
    "                squared_error = (outputs[j, :seq_len] - batch_labels[j, :seq_len]) ** 2\n",
    "                # 2. 计算单个样本的MSE，再开平方得到RMSE\n",
    "                rmse = torch.sqrt(torch.mean(squared_error))\n",
    "                batch_loss += rmse\n",
    "            \n",
    "            # 当前批次所有样本的平均RMSE\n",
    "            batch_loss /= len(batch_lengths)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "\n",
    "        loss_result_array.append(avg_loss)\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], RMSE Loss: {avg_loss:.4f}')\n",
    "        print(f'[{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}] Epoch [{epoch+1}/{num_epochs}], RMSE Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "EPOCHS = 30\n",
    "\n",
    "train_model(\n",
    "    model, \n",
    "    padded_train_sequences, \n",
    "    padded_train_labels, \n",
    "    train_lengths, \n",
    "    num_epochs=EPOCHS, \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练完成后绘制loss曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS+1), loss_result_array, 'b-o', linewidth=2)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, EPOCHS+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77db79",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
